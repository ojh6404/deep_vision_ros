defaults:
  - _self_
  - data: base
  - model: base
  - override hydra/job_logging: custom

hydra:
  run:
    dir: ./output/${exp_id}
  output_subdir: ${now:%Y-%m-%d_%H-%M-%S}-hydra

exp_id: default
debug: False
cudnn_benchmark: True
weights: null
checkpoint: null
seed: 14159265
acquire: True
num_workers: 16

log_text_interval: 100
log_image_interval: 1500
save_weights_interval: 10000
save_checkpoint_interval: 10000

pre_training:
  name: pre_training
  enabled: True
  batch_size: 16
  amp: False
  num_iterations: 80000
  learning_rate: 1.0e-4
  lr_schedule: constant
  point_supervision: True
  train_num_points: 8192
  oversample_ratio: 3.0
  importance_sample_ratio: 0.75
  clip_grad_norm: 3.0
  weight_decay: 0.001
  embed_weight_decay: 0.0
  backbone_lr_ratio: 0.1
  num_ref_frames: 2
  seq_length: 3
  num_objects: 1
  deep_update_prob: 0.2
  crop_size: [384, 384]
  frequent_save_in_last: 0
  frequent_save_interval: 1000

main_training:
  name: main_training
  enabled: True
  batch_size: 16
  amp: True
  num_iterations: ${data.main_training.num_iterations}
  learning_rate: 1.0e-4
  lr_schedule: step
  lr_schedule_steps: ${data.main_training.lr_schedule_steps}
  lr_schedule_gamma: 0.1
  point_supervision: True
  train_num_points: 12544
  oversample_ratio: 3.0
  importance_sample_ratio: 0.75
  clip_grad_norm: 3.0
  weight_decay: 0.001
  embed_weight_decay: 0.0
  backbone_lr_ratio: 0.1
  num_ref_frames: 3
  seq_length: 8
  num_objects: 3
  deep_update_prob: 0.2
  crop_size: [480, 480]
  merge_probability: 0.5
  max_skip_schedule: [5, 10, 15, 5]
  max_skip_schedule_fraction: [0.0, 0.1, 0.3, 0.8]
  frequent_save_in_last: 0
  frequent_save_interval: 1000
